15.09.2022 07:07

############################ CHAPTER 2 - INTELLIGENT AGENTS ##########################

- AI is any system that has the ability to think "rationally".
- Turing test approach
	- Requirements include NLP, knowledge representation, automated reasoning, machine learning, compution vision and speech recognition and robotics.
- Agent is just something that acts on the environment i.e it perceives env. through sensors and acts on env. through actuators.
- Computer agents operate autonomously, perceive their environment, persist over a prolonged time period, adapt to change and create and pursue goals.
- Rational agent is one that acts so as to achieve the best outcome or in case of uncertainty, the best expected outcome.
- Percept refers to the content an agent's sensors are perceiving.
- Percept sequence or percepts is the complete history of everything the agent has ever perceived.
- Agent function describes the behaviour of agent which maps any given percept sequence to an action.
- Agent program implements agent function.
- Agent function is an abstract mathematical description whereas agent program is a concrete implementation, running within some physical system.
- Conseqeuntialism means we evaluate an agent's behaviour by its consequences.
- Formal definition of rational agent
	- For each possible percepts, it should select an action that is expected to maximize its PM, given the evidence provided by the percepts and 
	whatever built-in knowledge the agent has.
- Task env. are the "problems" to which rational agents are the "solutions".
- PEAS - Performance, Env., Actuators and Sensors. This is description of the env.
- Fully Observable env. means agent's sensor give it access to ethe complete state of the env. at 
each point in time.
	- Need not maintain any internal state to keep track of the world.
- Single vs multi agent
- Deterministic env. means that the next state of env. is completely determined by the current state
and the action executed by the agent(s).
- Episodic env. means the agent's experience is divided into atomic episoed.
	- The next episode doesn't depend on the actions taken in previous episode.
- Dynamic env. is the env. that changes itself while an agnet is deliberating.
- Semidynamic means agent's performance score changed but env. itself doesn't change.
- Discrete vs continuous (time related)
- Known env. means that the outcomes for all actions are given or agent knows about the laws of physics
of the env.
- Agent architecture means a computing device with physical sensors and actuators on which agent 
program will run.
- Table size for table driven approach is sum(from 1 to T of |P|^t)
- Simple reflex agent selects actions on the basis of the current percept, ignoring the rest of the 
percept history.
	- Works only if env. is fully oberservable.
- Model-based reflex agents keep track of the part of the world it can't see now.
	- Done by maintaining some sort of internal state that depends on the percept history.
- Transition model is all about the knowledge about "how the world works".
- Sensor model is all about how the state of the world is reflected in the agent's percepts.
- Goal based agent require some sort of goal information that describes situations that are desirable 
in addition to current state description.
	- Uses search and planning
	- Less efficient but more flexible
- Utility based agent quantifies happiness along with reaching goal.
- An agent's utility function is internalization of the performance measure.
	- Usefull when there are conflicting or multiple goals
	- In real world, expected utility is maximized
- Any type of agent can be built into learning agent by teaching them.
- Learning element is responsible for making improvements
- Performance element is responsible for selecting external actions.
- The critic tells the learning element how well the agent is doing with respect to a fixed 
performance standard.
- Problem generator is responsible for suggesting actions that will lead to new and informative
experiences.
- Representations of agent programs
	- Atomic - World is indivisible, no internal structure.
	- Factored - Splits each state into a fixed set of variables or attributes, each of which
	can have a value.
	- Structures - Objects and their various and varying relationships are described explicitly.


15.09.2022 11:34

############################ CHAPTER 3 - SEARCH ########################################

- Problem-solving agent is an agent that "plans ahead" and considers a sequence of actions that
form a path to a goal state when the correct action to take is not immediately obvious.
	- Use atomic representations
	- Process used is called search
- Planning agents use factored or structured representations of states
- Informed search algorithms are those in which the agent can estimate how far it is from the goal.
- Uninformed search algorithms are opposite of informed search.
- 4 phases of problem-solving agent
	- Goal Formulation
	- Problem Formulation
	- Search
	- Execution
- Defintion of search problem
	- State Space - Set of all possible states that the env. can be in
	- Initial state
	- Set of Goal state
	- Actions
	- Transition model - What each action does (in short - result)
	- Action cost
- Path is sequence of actions and solution is a path from initial state to the goal state.
- Optimal solution has the lowest path cost among all solutions.
- Abstraction is the process of removing detail from a representation.
	- A good problem formulation has the right level of detail.
	- The choice of good abstraction involves removing as much detail as possible while retaining
	validity and ensuring that the abstract actions are easy to carry out.
- Standardized problem is illustrated or example problem whereas real world problem is one whose 
solutions people actually use.
- Each node in search tree corresponds to a state in the state space and edges correspond to actions.
- Root of the tree is initial state of the problem.
- State space describes set of states and the actions while search tree describes the paths between
these states, reaching towards the goal.
- Frontier nodes separates two regions of the state space graph.
	- Interior region where every state has been expanded.
	- Exterior region of states that have not yet been reached.
- Best-First Search - Choose a node with minimum value of evaluation function f(n).
- A search algorithm is called graph search if it checks for redundant paths and a tree like search
if it does not check.
- performance Measures for problem solving agents
	- Completeness - Ability of the algorithm to find a solution when there is one and to correctly
	report failure when there is not.
	- Cost Optimality
	- Time and Space Complexity
- BFS
	- Early goal test is checking whether a node is a solution as soon as it is generated.
	- Late goal test is waiting until a node is popped off the queue.
	- Always finds solution with a minimal number of actions.
	- Cost optimal if all actions have same cost.
	- Time and Space Complexity = O(b^d)
- Dijkstra or Uniform-cost search
	- Complete
	- Cost optimal in every case
	- Time and space Complexity = O(b^(1 + floor(C*/e))) where C* is optimal cost and e is
	lower bound on the cost of each action.
	- All action costs same, then it is essentially BFS.
- DFS
	- Incomplete
	- Space complexity = O(bm)
	- Time Complexity = O(b^m)
	- Not optimal
- Depth-limited
	- Version of DFS where we supply a depth limit, l and tread all nodes at depth l as if they
	had no successors.
	- Time complexity = O(b^l)
	- Space complexity = O(bl)
- Iterative deepening
	- Iteratively try different depth limiting values
	- Time complexity = either O(b^d) or O(b^m)
	- Space complexity = either O(bd) or O(bm)
- Hybrid Approach
	- Runs BFS until almost all memory is consumes and then runs iterative deepening from all nodes
	in the frontier.
- Bidirectional search (Self-explanatory)
- h(n) - Estimated cost of the cheapest path from the state at node n to a goal state.
- Greedy Best First Search
	- Expands first node with lowest h(n) value.
	- Evaluation function f(n) = h(n)
	- Complete in finite state space and not otherwise
	- Time and space complexity = O(b^m) but can be reduced to O(bm) with good heuristic
- A* Search
	- Evaluation function f(n) = g(n) + h(n)
	- Here, g(n) is the path cost from the initial state to node n and h(n) is the estimated cost
	of the shortest path from n to a goal state.
	- f(n) is the estimated cost of the best path that continues from n to a goal state.
	- Complete
	- Cost optimal if heuristic is admissible (one that never overestimates the cost to reach goal)
	- Consistency - h(n) <= c(n,a,n') + h(n')
	- Optimally effience as algorithm that extends search paths from initial state and uses the 
	same heuristic information, must expand all nodes that are surely expanded by A*
	- Prunes away search tree nodes that are not necessary for finding an optimal solution.
- Satisficing search
	- Willing to accept sub optimal search but are good enough.
	- Allow A* to use inadmissible heuristic
	- Eg. Detour index for routes --> Weighted A* f(n) = g(n) + W * h(n)
- Bounded Suboptimal search, bounded cost search, unbounded cost search and Memory bounded search
- Beam search
	- Limits the size of frontier and keeps only k nodes with the best f-scores.
	- Incomplete
	- Suboptimal
	- Alternatively - keep strict limit on the size of the frontier but instead keeps every
	node whose f-score is within delta of the best f-score.
- IDA*
	- Cut off is f-cost. At each iteration, the cut off value is the smallest f-cost of any node
	that exceeded the cutoff on the previous iteration.
- RBDS (Recursive best-first search)
	- Uses f_limit to keep track of f-score.
	- More efficient than IDA*
	- Optimal if heuristic is admissible.
- SMA* (Simplified Memory bounded A*) and bidirectional heuristic search
- Quality assurance for heuristic is through effective branching factor.
- A problem with fewer restrictions on the actions is called a relaxed problem.
- State space graph of the relaxed problem is a supergraph of the original state space because the
removal of restrictions creates added edges in the graph.
- Cost of an optimal solution to a relaxed problem is an admissible heuristic for the original problem.
- If a collection of admissible heuristics is available for a problem then choose the maximum of them.
